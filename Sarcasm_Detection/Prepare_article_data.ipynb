{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prepare_article_data.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1rQuLc-dzpiSx7lo6koRwCqg0eUtM1sJG","authorship_tag":"ABX9TyNMoHIKrZFcKzxRmPzyrCUe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bV7vCDL0p_1s"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import re\n","from urllib.request import Request, urlopen\n","from bs4 import BeautifulSoup\n","import html5lib\n","import pickle\n","import time\n","import progressbar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8RhkmeIfBzT"},"source":["Download the news headlines for sarcasm detection dataset from Kaggle https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home\n","\n","The dataset contains urls for the articles. Instead of just using the headlines, I wanted to pull some extra data from each of the articles. In particular, I pulled any text found within paragraph tags of the html for each article. The text data was cleaned to remove emojis and html tags."]},{"cell_type":"code","metadata":{"id":"jHTRYNBpqFHM"},"source":["import zipfile\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/Sarcasm/Sarcasm_Headlines_Dataset_v2.json.zip'\n","z = zipfile.ZipFile(path, 'r')\n","z.extractall('/tmp')\n","z.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbyOUwKMq23v"},"source":["import json\n","\n","def parse_data(file):\n","    for l in open(file,'r'):\n","        yield json.loads(l)\n","\n","data = list(parse_data('/tmp/Sarcasm_Headlines_Dataset_v2.json'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDy_Tt08rljH"},"source":["labels = []\n","headlines = []\n","urls = []\n","for record in data:\n","  labels.append(record['is_sarcastic'])\n","  headlines.append(record['headline'])\n","  urls.append(record['article_link'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhDXgre6BqKf","executionInfo":{"status":"ok","timestamp":1617917992600,"user_tz":240,"elapsed":259,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"2251b4ac-e80c-4a30-de5a-03ff12273d73"},"source":["len(urls)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28619"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"xuPEuBf3PaOq"},"source":["def deEmojify(text): # thank you https://stackoverflow.com/users/6579239/abdul-razak-adam\n","    regrex_pattern = re.compile(pattern = \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags = re.UNICODE)\n","    return regrex_pattern.sub(r'',text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpj9Zb982Mfl"},"source":["def clean_html(raw_html):\n","  cleanr = re.compile('<.*?>')\n","  cleantext = ''\n","  for x in raw_html:\n","    cx = re.sub(cleanr, '', str(x))\n","    cx = cx.replace(u'\\xa0', u' ')\n","    cx = cx.lstrip()\n","    cleantext += deEmojify(cx) + ' '\n","  return cleantext.lower() # the tokenizer converts to lowercase as a default"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FG0iB29xtNAq","executionInfo":{"status":"ok","timestamp":1617917699066,"user_tz":240,"elapsed":15916585,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"e5fd9f0a-84ed-4d27-c797-03a62951bcb6"},"source":["content = []\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/Sarcasm/content.pickle', 'wb') as f:\n","  pickle.dump(content, f)\n","\n","widgets = ['[',\n","         progressbar.Percentage(format='%(percentage)3d%%'),\n","         '] ',\n","           progressbar.Bar('#'),' (',\n","           progressbar.ETA(), ') ',\n","          ]\n","\n","for i in progressbar.progressbar(range(len(urls)), widgets = widgets):\n","  req = Request(urls[i], headers={'User-Agent': 'Mozilla/5.0'})\n","  try:\n","    stream = urlopen(req)\n","    html = stream.read()\n","    stream.close()\n","    failures.append(False)\n","    html = html.decode('utf-8') \n","    soup = BeautifulSoup(html, 'html5lib')\n","    content.append(clean_html(soup.find_all('h1') + soup.find_all('p')))\n","  except:\n","    content.append('Failed to retrieve: {}'.format(urls[i]))\n","    failures.append(True)\n","  if i % 1000 == 0:\n","    #reload\n","    with open('/content/drive/MyDrive/Colab Notebooks/Sarcasm/content.pickle', 'rb') as handle:\n","      C = pickle.load(handle)\n","    # saving\n","    with open('/content/drive/MyDrive/Colab Notebooks/Sarcasm/content.pickle', 'wb') as handle:\n","      pickle.dump(C+content, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    content = []"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[100%] |####################################################| (Time:  4:25:16) \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TfACxiOiz7jY"},"source":["#reload\n","with open('/content/drive/MyDrive/Colab Notebooks/Sarcasm/content.pickle', 'rb') as handle:\n","  C = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFQY3esLaS35"},"source":["# saving the remaining stuff not saved in loops above\n","with open('/content/drive/MyDrive/Colab Notebooks/Sarcasm/content.pickle', 'wb') as handle:\n","  pickle.dump(C+content, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9eqT_APacFy5"},"source":[""],"execution_count":null,"outputs":[]}]}